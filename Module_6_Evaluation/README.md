##### DAE code reference: https://github.com/tagoyal/factuality-datasets

##### PARENT code reference: https://github.com/google-research/language/tree/master/language/table_text_eval

Traditional evaluation methods:

    BLEU
  
    ROUGE
  
    METEOR

Evaluation methods which are chosen with the intention to measure halucination level:

    PARENT - should be run from command line
    DAE

Run preprocess_T5_output.py for the output files which are generated by T5 verbalizers

Run preprocess_distilGPT2_output.py for the output files which are generated by DistilGPT2 verbalizers

Run extract_triples_4_PARENT.py to extract triples (3 versions: triples, triples + wiki, triples + wiki desc only)

Run calculate_BLEU_ROUGE_METEOR.py - results will be in 'results' folder

For PARENT scores:

    run table_test_eval.py from command line. remember to adjust the path to the generated text file
    record the results to the txt files which start with 'parent_results_of_' in the 'PARENT_results' folder

For DAE scores:
    
    Start a CoreNLP server as follows:
        java -mx4g -cp "*" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000
    run prepare_my_outputs.py - resulting files will be located in 'DAE/prepared_data' folder
    run evaluate_generated_outputs.py - results will be in 'DAE_results'
    
For T2G evaluation: 

    Run evaluate_T2G_output_of_CycleGT.py
    
For selecting data for error analysis:

    run data_selection_4_error_analysis.py
    
For getting the results of human annotation study:
    
    run .py in the 
    
Imported note: A few generation instances contain issues like being too short or too long. Most of the issues are gone after running prepare_my_outputs.py but a few samples are corrected manually. You may find those instances by searching 'filler filler filler filler' string.

